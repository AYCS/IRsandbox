[{"id":715392,"title":"Setting up a RAID 1 NAS with a seperate OS HDD?","body":"<p>So I'm a solid ubuntu-amateur with only a year of experience or so hoping to put together an ubuntu-samba file server and looking to learn more. I have seen a couple tutorials however the tutorials always show someone installing the OS on the same drive as the data which takes up space. I'm looking to understand what is more efficient in terms of processing. \nI'm hoping to run a 320GB HDD for the OS (Ubuntu Server 14.04)\na WD 3.0TB as the primary disk with another 3.0TB disk as the mirror. \nIf I'm understanding this right; would the 3.0TB drive act as a \"data/file\" extension to the 320GB storing the OS and then the secondary 3.0TB drive just be mirroring the files of the first 3.0TB?</p>\n\n<p>I have installed mdadm and played around following a couple tutorials however I can't seem to get it to partition correctly. I read that you cant use fdisk because the drive is larger than 2.0TB, so you must use GNU parted. </p>\n\n<p>If anyone has any better links or guides I'd love to know them! I've actually never posted in these forums before so I'm learning A LOT this week! I have been looking through other questions and forums for about 2 days to answer this. Just looking to get some better insight from anyone who might have prior experience or thinks I'm doing it wrong. (probably the case) Thanks in advance! </p>\n","related_questions":[{"id":710343,"title":"Building a light weight home server/NAS. Question on RAID and filesystems","body":"<p>I'm planning a build that will be using Ubuntu Server 14.04. I have been trying to learn as much as possible and practice in virtualbox before actually installing any hardware. I am really enjoying learning Ubuntu and I think I am nearly ready.</p>\n\n<p>I will likely use 3x3TB Reds with the OS on a smaller SSD. I would like to be able to share files with this box via windows (samba). </p>\n\n<ol>\n<li>I am highly considering RAID 5 for this setup. If I ever need to expand the size of this  storage pool, is adding another RAID pool (say 3x3 again) the best option?</li>\n<li>For the initial partitioning/formatting of the drives, is this best done through the installation menu or by using mdadm?</li>\n<li>Lasly, is NTFS the best option for the filesystem of these drives? My understanding is that EXT systems can only be used on linux machines.</li>\n</ol>\n\n<p>Thank you in advance for any guidance. </p>\n"},{"id":304672,"title":"How to re-add accidentally removed hard drive in RAID5","body":"<p>I have a NAS on Ubuntu Server with 4 2TB hard drives in RAID 5. A couple of weeks ago, one of the hard drives died, but my RAID was working, although degraded. Luckily it was still under warranty and I was sent a new hard drive which I installed today. However, when trying to add the new hard drive into the RAID, it was not rebuilding. So I unplugged the hard drive and rebooted the machine. However, I accidentally set one of my OTHER hard drives in the RAID to fail and removed it using mdadm.</p>\n\n<p>Now it says my RAID has two removed hard drives. I still have my 3rd hard drive with all my data still intact, but I don't know how to re-add it back into the RAID array, so it's back to a good (although degraded) state, so I can continue to add the 4th hard drive and rebuild the array. Is it possible to just have Ubuntu realize that the 3rd hard drive has my data and just have it recognized as part of the array again?</p>\n\n<p>When I try to run: </p>\n\n<pre><code>sudo mdadm --manage /dev/md127 --re-add /dev/sdd1 \n</code></pre>\n\n<p>It says:</p>\n\n<pre><code>mdadm: --re-add for /dev/sdd1 to dev/md127 is not possible\n</code></pre>\n\n<p>Please, any help that anyone can give would be much, much appreciated.</p>\n"},{"id":672188,"title":"Replace failed disk in RAID 1","body":"<p>two days ago one of my two drives in my software RAID 1 (that was setup using gnome-disks) configuration failed. I could not access the files in the meantime. After short research it seems that I have to replace the failed disk and rebuild the RAID to access my files again. So no problem today I got my replacement disk, removed the old broken drive from the raid and tried to add the new drive to the disk. </p>\n\n<p>(Before I copied the partition layout from the working drive with: <code>sfdisk -d /dev/sdb | sfdisk /dev/sda</code>)</p>\n\n<p>When I try to add a new disk to my RAID (that shows up in my gnome-disks) with <code>mdadm —manage /dev/md0 —add /dev/sda</code>, I run into this error:</p>\n\n<pre><code>mdadm: Cannot get array info for /dev/md0\n</code></pre>\n\n<p>The same thing happens to basically any operation using <code>mdadm</code> on my /dev/md0.</p>\n\n<p><strong>How can I get my RAID back working?</strong></p>\n\n<p>Some info:</p>\n\n<p>Ubuntu 15.04 (64bit)</p>\n\n<pre><code>mdadm --version\n</code></pre>\n\n<p><em>mdadm - v3.3 - 3rd September 2013</em></p>\n\n<pre><code>uname -a\n</code></pre>\n\n<p><em>Linux Z97X 3.19.0-27-generic #29-Ubuntu SMP Fri Aug 14 21:43:37 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux</em></p>\n\n<pre><code>cat /proc/mdstat\n\nPersonalities : [linear] [multipath] [raid0] [raid1] [raid6] [raid5] [raid4] [raid10] \nmd0 : inactive sdb[0](S)\n      976627383 blocks super 1.2\n\nunused devices: &lt;none&gt;\n\nmdadm -D /dev/md0\n/dev/md0:\n        Version : 1.2\n     Raid Level : raid0\n  Total Devices : 1\n    Persistence : Superblock is persistent\n\n          State : inactive\n\n           Name : PC:0\n           UUID : XXXXXXXX:7af47836:XXXXXXX:d7360235\n         Events : 3569\n\n    Number   Major   Minor   RaidDevice\n\n       -       8       16        -        /dev/sdb\n</code></pre>\n\n<p><em>Why is <strong>Raid Level : raid0</strong> when I setup this as raid1 in gnome-disks / it's shown as raid1 in gnome disks?</em></p>\n\n<blockquote>\n  <p>And one question in general: My understanding when I put up my RAID 1\n  was that if one disk fails I can still access my data because data is\n  mirrored. But when the disk failed this was obviously not the case.\n  Why? Is my understanding of RAID 1 wrong?</p>\n</blockquote>\n"},{"id":653270,"title":"How to change raid1 to raid 11?","body":"<p>raid1  --> /dev/sdb   /dev/sdc</p>\n\n<p>raid11 --> /dev/sdd   /dev/sde</p>\n\n<p>/dev/sdd   -> bad sector \n/dev/sde   -> new hard disk</p>\n\n<p>I have for hard disk. And I have created two raid in that with raid array <strong>raid1</strong> and <strong>raid11</strong>. In which raid11 has one faulty hard disk(/dev/sde) and one hard disk has bad sector(/dev/sdd). </p>\n\n<p>So while I am syncing /dev/sdd with new hard disk then it is not syncing so I have remove two hard disk of raid1. And create raid with sde as raid1. So now I have raid1 with one hard disk(/dev/sde) and raid11 with one hard disk(/dev/sdd) sdd has bad sector. And then I have taken data from raid11 to raid1.  </p>\n\n<p>So I have two raid1. One is that I have remove and other is backup of raid11. How can I rename raid1 to raid11 Is it possible?</p>\n"},{"id":593865,"title":"How to start RAID array?","body":"<p>a few months ago two volumes in my Synology DiskStation 412+ NAS crashed and until now I am still unable to recover the data but I am motivated and won't give up.</p>\n\n<p>I am able to see the hard drives (sda and sdc) under \"disks\" in Ubuntu but I am unable to mount the raid volume.</p>\n\n<p>It also detects the RAID array but it is not running.</p>\n\n<p>If I want to start it, then it says \"No array found in config file or automatically\"\nWhen I try the command \"sudo mdadm -A /dev/sda /dev/sdc\" then it says it exists but it is not an md array. </p>\n\n<p>At this point I don't know what to do. Any help will be appreciated. You can find all screenshots <a href=\"https://www.dropbox.com/sh/ksrsd92n9vooa6i/AACG7TLXDgFFhK4U9eqKatW9a?dl=0\" rel=\"nofollow\">here</a></p>\n\n<p>Since I am new to this forum I am not allowed to upload pictures or add more than 2 links so I created a photo album on Dropbox with all screenshots.</p>\n\n<p>Thanks for your help in advance! I hope I can recover my data soon!</p>\n"},{"id":548405,"title":"Serious RAID problem that panics system","body":"<p>System: Dual Xeon E5-2630 CPU, 32GB RAM, Primary disk is a SATA-III 512GB Crucial SSD\nOS: Xubuntu 14.04.1</p>\n\n<p>I am having a serious problem with RAID on this new system and hope some of you can provide some insight. Currently, the primary SSD with the root filesystem is not mirrored, although I plan to mirror it to a second identical SSD in the future. I am attempting to set up a RAID on a secondary HDD set and am unwilling to upgrade the primary SSD set until this problem is solved.</p>\n\n<p>I have a pair of SATA-III Seagate ST4000DM0004TB Baracuda 4TB drives in this system which were formatted identically with a single large ext4 GPT partition. I have been attempting to create a useful RAID 1 mirror on these disks which is then mounted on /x. At one point I had something that appeared to be stable and ran for a few weeks until I tried to modify the Array, at which point it failed. Every time the mirror fails, it apparently panics the system and the root filesystem on the SSD is remounted read-only as per the setting in /etc/fstab (errors=remount-ro). Of course the system is now useless and requires a hard reset. The system reboots but the mirror is now totally corrupted and must usually be destroyed and rebuilt. I've run hardware diagnostics and see no problem. There are zero hints as to what is wrong in any of the log files (dmesg, kern.log, syslog). Here are some details:</p>\n\n<hr>\n\n<p>I create the Array as follows:</p>\n\n<pre><code># mdadm --create /dev/md2 --verbose --level=1 --raid-devices=2 /dev/sdc1 /dev/sdd1\nmdadm: /dev/sdc1 appears to contain an ext2fs file system\n    size=-387950592K mtime=Wed Dec 31 16:00:00 1969\nmdadm: Note: this array has metadata at the start and\n    may not be suitable as a boot device. If you plan to\n    store '/boot' on this device please ensure that\n    your boot-loader understands md/v1.x metadata, or use\n    --metadata=0.90\nmdadm: /dev/sdd1 appears to contain an ext2fs file system\n    size=-387950592K mtime=Wed Dec 31 16:00:00 1969\nmdadm: size set to 3906885440K\nContinue creating array? y\nmdadm: Defaulting to version 1.2 metadata\nmdadm: array /dev/md2 started.\n</code></pre>\n\n<p>I check the RAID build progress:</p>\n\n<pre><code># cat /proc/mdstat\nPersonalities : [linear] [multipath] [raid0] [raid1] [raid6] [raid5] [raid4] [raid10]\nmd2 : active raid1 sdd1[1] sdc1[0]\n    3906885440 blocks super 1.2 [2/2] [UU]\n    [&gt;....................] resync = 0.4% (17314560/3906885440) finish=415.6min speed=155968K/sec\n\nunused devices: &lt;none&gt;\n</code></pre>\n\n<p>I continue to periodically monitor the resync operation with the above command and it moves along without problem. However, at some point (I've had the resync get to anywhere from 4% to 60% synced), the system panics and root is remounted RO. When the system is rebooted, I usually find the following:</p>\n\n<pre><code># l /dev/md*\n/dev/md127 /dev/md127p1\n\n/dev/md:\ndymaxion:2@ dymaxion:2p1@\n</code></pre>\n\n<p>In the case where I did manage to get /dev/md2 built and running, I had /dev/md2 and /dev/md2p1 devices with nothing in the /dev/md subdirectory. Here the panicked system seems to try to salvage the array as md127. I do not understand why, but this has happened repeatedly. Possibly it is the result of some algorithm coded into the mdadm software.</p>\n\n<p>Sometime the md127 array is degraded to such a point that it cannot be mounted at boot time (there is an entry for the array in /etc/fstab) and other times it does mount and attempt to resync. However, it often panics the system during this operation, leading to a continual series of reboots.</p>\n\n<p>I then destroy the array and attempt to recreate it. These are the commands I use to destroy it.</p>\n\n<pre><code># mdadm --stop /dev/md127\nmdadm: stopped /dev/md127\n\n# cat /proc/mdstat\nPersonalities : [linear] [multipath] [raid0] [raid1] [raid6] [raid5] [raid4] [raid10]\n</code></pre>\n\n<p>unused devices: </p>\n\n<pre><code># mdadm -QD /dev/md127\nmdadm: cannot open /dev/md127: No such file or directory\n\n# mdadm --zero-superblock /dev/sdc1\n# mdadm --zero-superblock /dev/sdd1\n</code></pre>\n\n<p>I have tried building the array on a quiet system with no desktop processes running other than a few terminal windows. I've run the checkbox-gui hardware test suite and everything checks out fine. I tried unplugging all other SATA disks, USB ports, Card Reader, Optical Disk, etc. and then run the build and it still fails.</p>\n\n<p>Can anyone spot some reason why the array is failing or suggest some way to better determine what is happening?</p>\n\n<p>Here is some additional information on my efforts.</p>\n\n<p>What I have been doing on my Sun Server (Solaris 10) for the past 10 years is attach a third disk to the array, allow it to sync up, detach it from the array and then take it off site for disaster recovery. This has been working great and this is what I planned to do on this Ubuntu system.</p>\n\n<p>Using the above procedures, I did once manage to get /dev/md2 properly built with the two internal disks. The system ran without problem for a few weeks, so I was ready to attach the third disk using a hot-swap bay. I rebooted with the 3rd disk in the hot-swap bay. Because of the arbitrary device reassignments, the new disk appeared as /dev/sda and the mirror was using /dev/sdd and /dev/sde.</p>\n\n<pre><code># mdadm -QD /dev/md2p1 (or: # mdadm -QD /dev/md2)\n/dev/md2:\n        Version : 1.2\n  Creation Time : Tue Sep 9 17:50:52 2014\n     Raid Level : raid1\n     Array Size : 3906885440 (3725.90 GiB 4000.65 GB)\n  Used Dev Size : 3906885440 (3725.90 GiB 4000.65 GB)\n   Raid Devices : 2\n  Total Devices : 2\n    Persistence : Superblock is persistent\n\n    Update Time : Fri Sep 19 14:02:45 2014\n          State : clean\n Active Devices : 2\nWorking Devices : 2\n Failed Devices : 0\n  Spare Devices : 0\n\n           Name : dymaxion:2 (local to host dymaxion)\n           UUID : 1e131e20:9c899b31:a7494bc5:1dbc253f\n         Events : 129\n\n      Number Major Minor RaidDevice State\n         3 8 65 0 active sync /dev/sde1\n         2 8 49 1 active sync /dev/sdd1\n\n\n# cat /proc/mdstat\nPersonalities : [linear] [multipath] [raid0] [raid1] [raid6] [raid5] [raid4] [raid10]\nmd2 : active raid1 sdd1[2] sde1[3]\n      3906885440 blocks super 1.2 [2/2] [UU]\n\nunused devices: &lt;none&gt;\n</code></pre>\n\n<p>Everything looks good. Let's add /dev/sda1 as a spare to /dev/md2p1:</p>\n\n<pre><code># mdadm /dev/md2 --add /dev/sda1\n\n# mdadm -QD /dev/md2\n/dev/md2:\n        Version : 1.2\n  Creation Time : Tue Sep 9 17:50:52 2014\n     Raid Level : raid1\n     Array Size : 3906885440 (3725.90 GiB 4000.65 GB)\n  Used Dev Size : 3906885440 (3725.90 GiB 4000.65 GB)\n   Raid Devices : 2\n  Total Devices : 3\n    Persistence : Superblock is persistent\n\n    Update Time : Fri Oct 17 13:36:13 2014\n          State : clean\n Active Devices : 2\nWorking Devices : 3\n Failed Devices : 0\n  Spare Devices : 1\n\n           Name : dymaxion:2 (local to host dymaxion)\n           UUID : 1e131e20:9c899b31:a7494bc5:1dbc253f\n         Events : 130\n\n      Number Major Minor RaidDevice State\n         3 8 65 0 active sync /dev/sde1\n         2 8 49 1 active sync /dev/sdd1\n\n         4 8 1 - spare /dev/sda1\n</code></pre>\n\n<p>OK, let's attach the spare to the array using the grow option:</p>\n\n<pre><code># mdadm /dev/md2 --grow -n3\n\n# mdadm -QD /dev/md2\n/dev/md2:\n        Version : 1.2\n  Creation Time : Tue Sep 9 17:50:52 2014\n     Raid Level : raid1\n     Array Size : 3906885440 (3725.90 GiB 4000.65 GB)\n  Used Dev Size : 3906885440 (3725.90 GiB 4000.65 GB)\n   Raid Devices : 3\n  Total Devices : 3\n    Persistence : Superblock is persistent\n\n    Update Time : Fri Oct 17 14:43:08 2014\n          State : clean, degraded, recovering\n Active Devices : 2\nWorking Devices : 3\n Failed Devices : 0\n  Spare Devices : 1\n\n Rebuild Status : 0% complete\n\n           Name : dymaxion:2 (local to host dymaxion)\n           UUID : 1e131e20:9c899b31:a7494bc5:1dbc253f\n         Events : 134\n\n      Number Major Minor RaidDevice State\n         3 8 65 0 active sync /dev/sde1\n         2 8 49 1 active sync /dev/sdd1\n         4 8 1 2 spare rebuilding /dev/sda1\n</code></pre>\n\n<p>Looks good! Allow the third disk to sync:</p>\n\n<pre><code># cat /proc/mdstat\nPersonalities : [linear] [multipath] [raid0] [raid1] [raid6] [raid5] [raid4] [raid10]\nmd2 : active raid1 sda1[4] sdd1[2] sde1[3]\n      3906885440 blocks super 1.2 [3/2] [UU_]\n      [&gt;....................] recovery = 0.7% (27891328/3906885440) finish=376.2min speed=171823K/sec\n\nunused devices: &lt;none&gt;\n</code></pre>\n\n<p>Somewhere after the mirror had synced more than 10%, the system panicked. This time, when the system was rebooted, the boot process was unable to reattach the mirror to /x and prompted to retry or skip the mount. I skipped it and when the system booted, there was no way to reactivate /dev/md2. Ultimately I had to destroy it and start over. I never got this close again. had this worked, the plan was to mark the third disk as failed, remove it and grow the array back to two devices (or two devices and a missing spare.)</p>\n\n<p>Do you see anything wrong with any of this build procedure?</p>\n\n<p>I apologize for the long post. I wanted to try and provide as much information as possible to try and anticipate any questions.</p>\n\n<p>Any suggestions are greatly appreciated. I am particularly concerned about what is causing the system to panic.</p>\n\n<hr>\n\n<p><strong>Everything below here was added Saturday, November 15, 2014</strong></p>\n\n<hr>\n\n<p>First, let me clarify an apparent misunderstanding. @psusi wrote:</p>\n\n<blockquote>\n  <p>Since you didn't mention creating a filesystem on the raid array and\n  mounting it after creating the array, and mdadm warned you that\n  /dev/sdc1 already has an ext2 filesystem in it, I'm guessing you mean\n  you already have a filesystem in /dev/sdc1, and that is what is being\n  remounted read only.</p>\n</blockquote>\n\n<p>No.  The root filesystem is on its own solid state SATA-III drive (sda1) while I am attempting to build the md2 mirror using two other 4TB disks (sdc and sdd).  It is while this mirror is syncing that something goes wrong, the entire system panics, and it is the root filesystem, not the mirror, that gets remounted read-only, making the entire OS non-operative and requiring a hard reset.  Upon reboot, the mirror is apparently attempted to be reconstructed but is now typically named /dev/md127.</p>\n\n<p>Yes, I was attempting to create the mirror using two disks that had previously been partitioned with a GPT partition table and then formatted with one large ext4 filesystem.  From everything I have read, this should be acceptable.</p>\n\n<blockquote>\n  <p>[NOTE: When mdadm says \"/dev/sdd1 appears to contain an ext2fs file\n  system\", it is misidentifying the ext4fs -- probably due to a\n  hard-coded error message that was never properly updated.  As far as\n  the partition types, GParted does not allow them to be formatted as\n  type fd directly, but I do believe that mdadm tags them as such when\n  it assembles them into the array.]</p>\n</blockquote>\n\n<p>Based upon the comments below, this is what I tried:</p>\n\n<p>1: I ran an extended S.M.A.R.T. surface test on all four 4TB drives (2 for mirror, 2 as future spares).  Each test took over 8.5 hours and all disks reported without error.  Exercising these disks individually has never caused a system panic.</p>\n\n<p>2: Using GParted, I deleted the ext4 partitions from the sdc and sdd disks.</p>\n\n<p>3: To make sure that the original GPT partition tables were eliminated I ran:</p>\n\n<pre><code># sgdisk -Z /dev/sdc\n# sgdisk -Z /dev/sdd\n</code></pre>\n\n<p>4: I recreated the array using the two unformatted disks.</p>\n\n<pre><code># mdadm --create /dev/md2 --verbose --level=1 --metadata 1.2 --raid-devices=2 /dev/sdc /dev/sdd\nmdadm: size set to 3906887360K\nmdadm: array /dev/md2 started\n</code></pre>\n\n<p>5: I started monitoring the sync using \"cat /proc/mdstat\" and saw it advancing nicely.</p>\n\n<p>After a couple of minutes the system panicked as usual and the root filesystem (sda1) was remounted RO and required a hard reset.  Upon reboot the array was renamed /dev/md127 and in this case, it is in a \"resync=PENDING\" state and is not automatically attempting to sync.  The intent was to create the GPT partition table and ext4 partition on the mirror once it finished syncing.  (I know that I could have probably gone ahead and done this during the sync, but I am trying to isolate the steps in this process to see where the problem lies.)</p>\n\n<p>Here is some new information that I found duplicated in the syslog and kern.log files.  These messages were logged just prior to the remount-ro operation.</p>\n\n<pre><code>Nov 15 14:31:15 dymaxion kernel: [58171.002154] ata8.00: exception Emask 0x0 SAct 0x0 SErr 0x0 action 0x6 frozen\nNov 15 14:31:15 dymaxion kernel: [58171.002163] ata8.00: failed command: IDENTIFY DEVICE\nNov 15 14:31:15 dymaxion kernel: [58171.002167] ata8.00: cmd ec/00:01:00:00:00/00:00:00:00:00/00 tag 16 pio 512 in\nNov 15 14:31:15 dymaxion kernel: [58171.002167]          res 40/00:ff:00:00:00/00:00:00:00:00/00 Emask 0x4 (timeout)\nNov 15 14:31:15 dymaxion kernel: [58171.002169] ata8.00: status: { DRDY }\nNov 15 14:31:15 dymaxion kernel: [58171.002175] ata8: hard resetting link\nNov 15 14:31:15 dymaxion kernel: [58171.329795] ata8: SATA link up 6.0 Gbps (SStatus 133 SControl 300)\nNov 15 14:31:15 dymaxion kernel: [58171.330336] ata8.00: supports DRM functions and may not be fully accessible\nNov 15 14:31:15 dymaxion kernel: [58171.334346] ata8.00: disabling queued TRIM support\nNov 15 14:31:15 dymaxion kernel: [58171.339116] ata8.00: supports DRM functions and may not be fully accessible\nNov 15 14:31:15 dymaxion kernel: [58171.343149] ata8.00: disabling queued TRIM support\nNov 15 14:31:15 dymaxion kernel: [58171.347557] ata8.00: configured for UDMA/133\nNov 15 14:31:15 dymaxion kernel: [58171.347625] ata8: EH complete\n</code></pre>\n\n<p>This seems to indicate some sort of SATA error, although, at the moment I cannot interpret it.</p>\n\n<p>So, does this provide any additional clues as to what may be wrong?  I truly appreciate the help thus far.  It got me thinking in a couple new directions.  I hope someone can provide further insight or suggestions.  Thanks.</p>\n\n<hr>\n\n<p><strong>Everything below here was added Saturday, December 20, 2014</strong></p>\n\n<hr>\n\n<p>As a final entry in this saga, I am providing the following information in the hopes that it will help others in the future.</p>\n\n<p>I did manage to get in touch with US ASUS support regarding this problem.  I received a replacement Z9PE-D8 WS motherboard which I installed and configured.  When I ran my RAID tests I ended up observing exactly the same results as with the original motherboard.  With the root filesystem drive attached to the Marvel controller:</p>\n\n<ul>\n<li><p>If the additional RAID 1 disks were on the Marvel controller, any attempt to perform a significant mdadm(8) operation on the array generated the kernel exception and errors noted above and the entire OS would panic. </p></li>\n<li><p>If the RAID disks were moved off of the Marvel controller, then mdadm(8) operations could be performed without problem and the system operated without problem.</p></li>\n</ul>\n\n<p>Since I intended to mirror the root partition, I was anxious to see what would happen if the root filesystem was removed from the Marvel controller and the RAID was moved back onto it.  Unfortunately, I could find no way to ever boot the OS if the root filesystem was moved to the on-board Intel C602 chipset.  This was the case with both motherboards. </p>\n\n<p>[NOTE:  If anyone has a clue why this could not be done, I would appreciate hearing the reason.  For example, does GRUB2 store some particular information at installation time that is controller-specific?]</p>\n\n<p>Therefore, I bit the bullet and decided to completely reinstall the latest Ubuntu Server version 14.10 and mirror the root filesystem as part of the install process.  I moved the SSDs to the pair of SATA-III ports controlled by the Intel controller and performed a fresh install.  Everything worked fine.</p>\n\n<p>Now, with a running system with a mirrored root, I attached the two 4TB drives to the Marvel controller and tried to construct a new RAID 1 array.  The array soon failed.  Thus, we can conclusively conclude that the Marvel controller is doing something that is incompatible with software RAID management.</p>\n\n<p>I moved the 4TB drives to SATA-II ports controlled by the Intel C602 and everything worked and continues to work without a hitch.  ASUS engineering is looking into the problem while I am left with a machine where four of the original six SATA-III ports are unusable.</p>\n\n<p><strong>The lesson is that anyone considering a Linux machine that uses Marvel PCIe 9230 controller should be concerned about RAID compatibility.</strong></p>\n\n<p>I hope this information is useful.  If anyone else discovers similar problems with the Marvel controller and can shed further light on the subject, please contact me.  Thanks. \n~                  </p>\n"},{"id":520210,"title":"OVH - Dedi 14.04 Server - Raid Reconfiguration Help for a Total Noob","body":"<p>I am a linux noob and have been saddled with a problem far beyond my abilities. </p>\n\n<p><strong>My situation is this:</strong></p>\n\n<ul>\n<li><strong>This box</strong>: <a href=\"http://www.soyoustart.com/us/offers/bk-24t.xml\" rel=\"nofollow\">http://www.soyoustart.com/us/offers/bk-24t.xml</a> (6x4TB\ndrives) </li>\n<li><p><strong>OVH image based install of Ubuntu 14.04 server</strong> - I do not have access to installation as I would if I were installing from DVD. I tell the CP what OS I want installed and it returns an installed system based on their configuration. I do have some options, one of which is <em>use distribution kernel</em>. I can also choose a partition scheme via the web but it only exposes the first drive. I can choose to <em>install all on one disk</em> which would put /, /swap and /home all on that first disk. </p></li>\n<li><p><strong>RAID info after their OS install</strong>: <a href=\"http://pastebin.com/hGnK9Znp\" rel=\"nofollow\">http://pastebin.com/hGnK9Znp</a> (Asking in the official #ubuntu channel I am told that: \"that's 1 disk with 5 mirrors and zero spares. It has 5 allocated mirrors - it's a very bad config\".) <strong>Edit:</strong> Is it possible they have just screwed up when setting up the raid? Could it just be down to human error on their part?</p></li>\n</ul>\n\n<p><strong>Desired Outcome:</strong></p>\n\n<ul>\n<li><p><strong>Raid1 system and swap disk</strong>  Do I have to allocate a whole disk to this or can I use part of one? Sys and swap will never need 4TB will they? As I understand it, and if a whole disk is used, this would be 4Tb storage and 1 drive redundancy (eg a mirror of that drive).</p></li>\n<li><p><strong>Raid5 for the remaining 4 drives as one big volume</strong> on which /home lives.  As I understand it, with raid5 this would provide 12TB of storage and single drive redundancy.</p></li>\n</ul>\n\n<p><strong>Obstacles</strong> </p>\n\n<ol>\n<li><p><strong>I am not a linux geek</strong></p></li>\n<li><p><strong>I am unable to alter the way the OS is installed.</strong> I do have a 'recovery mode' which loads an image via the net and allows me to work with the various systems on the installed OS. I can, for example, run <em>parted</em> but apt-get doesn't work.</p></li>\n<li><p><strong>I have no idea how to get from the RAID array listed above to the config I have described.</strong></p></li>\n<li><p><strong><em>I am stuck with this server and this provider.</em></strong></p></li>\n</ol>\n\n<p><strong><em>Complete loss of data is fine if I end up where I need to be.</em></strong></p>\n\n<p>I have looked at some official documentation but it seems out of date or is otherwise inapplicable. Specifically these resources:</p>\n\n<ul>\n<li>help.ubuntu.com/community/Installation/SoftwareRAID</li>\n<li>help.ubuntu.com/community/FakeRaidHowto</li>\n<li>www.tldp.org/HOWTO/LVM-HOWTO/</li>\n</ul>\n\n<p><strong>Edit 2: The advice I got from #Ubuntu is:</strong> <em>\"basically, break the 5 mirrors, create new raid 5/6 array out of them, that is it (if you don't want to touch the root disk). Once thats done, remove the remaining 1 disk raid 1 out of raid configuration as there are no more mirrors for it, you need to remove it. I advise you to mark the 5 mirrors as \"failed\" before removing them so it thinks the disks have failed before you disable them.</em></p>\n\n<p>While that makes sense conceptually, the practical application of that advice is beyond my current understanding.</p>\n\n<p>I've tried to be comprehensive in describing my problem, but I am sure I have left salient details out. Please ask for clarification where needed. </p>\n\n<p>Thanks :D</p>\n\n<hr>\n\n<p><strong>Update 2014.09.05</strong>\nI have re installed the OS using the <em>Custom Installation</em> option and the Install Entirely On First Disk option in the partition scheme dialogue.  This leaves the remaining drives completely untouched. This means I should be able to set them up as a new raid array instead of having to undo the default raid array as installed.</p>\n"},{"id":404848,"title":"RAID 5 won&#39;t mount","body":"<p>I have a qnap NAS which is running a version of ubuntu and I'm not getting any help from their forum so I thought I'd ask my question here.</p>\n\n<p>I \"had\" 3 2TB HD's in a raid 5 configuration.  When I got back from my holiday vacation, I found that the raid volume wasn't mounted.  I ran SMART utility from the QNAP admin page and it detected read errors on Disk2 so I bought another 2TB disk, did a swap and it looked like it was going to fix my problem:</p>\n\n<pre><code>[~] # cat /proc/mdstat\nPersonalities : [linear] [raid0] [raid1] [raid10] [raid6] [raid5] [raid4] [multipath]\nmd0 : active raid5 sdd3[3] sdc3[2] sda3[0]\n      3903891200 blocks level 5, 64k chunk, algorithm 2 [3/2] [U_U]\n      [========&gt;............]  recovery = 41.1% (802441984/1951945600) finish=285.7min speed=67047K/sec\n</code></pre>\n\n<p>I went to bed and let it finish rebuilding.  The next morning I tried to create the array and it failed:</p>\n\n<pre><code>[~] # mdadm -CfR --assume-clean /dev/md0 -l 5 -n 3 /dev/sdb3 /dev/sda3 /dev/sdc3\nmdadm: /dev/sdb3 appears to contain an ext2fs file system\n    size=-391076224K  mtime=Thu Jan  2 03:30:10 2014\nmdadm: /dev/sdb3 appears to be part of a raid array:\n    level=raid5 devices=3 ctime=Tue Jan  7 10:01:03 2014\nmdadm: /dev/sda3 appears to contain an ext2fs file system\n    size=-403668988K  mtime=Sat Jan 15 17:45:38 2011\nmdadm: /dev/sda3 appears to be part of a raid array:\n    level=raid5 devices=3 ctime=Tue Jan  7 10:01:03 2014\nmdadm: /dev/sdc3 appears to be part of a raid array:\n    level=raid5 devices=3 ctime=Tue Jan  7 10:01:03 2014\nmdadm: array /dev/md0 started.\n\n[~] # mount /dev/md0 /share/MD0_DATA -t ext4\nmount: wrong fs type, bad option, bad superblock on /dev/md0,\n       missing codepage or other error\n       In some cases useful info is found in syslog - try\n       dmesg | tail  or so\n\n[~] # dmesg | tail\n[ 5474.498500] EXT4-fs (md0): ext4_check_descriptors: Block bitmap for group 1952 not in group (block 224078240)!\n[ 5474.504503] EXT4-fs (md0): group descriptors corrupted!\n</code></pre>\n\n<p>I ran e2fsck and it complained about the superblock:</p>\n\n<pre><code>[~] # e2fsck -f /dev/md0\ne2fsck 1.41.4 (27-Jan-2009)\ne2fsck: Group descriptors look bad... trying backup blocks...\ne2fsck: Filesystem revision too high while trying to open /dev/md0\nThe filesystem revision is apparently too high for this version of e2fsck.\n(Or the filesystem superblock is corrupt)\n\n\nThe superblock could not be read or does not describe a correct ext2\nfilesystem.  If the device is valid and it really contains an ext2\nfilesystem (and not swap or ufs or something else), then the superblock\nis corrupt, and you might try running e2fsck with an alternate superblock:\n    e2fsck -b 8193 &lt;device&gt;\n</code></pre>\n\n<p>I tried alternate superblocks:</p>\n\n<pre><code>[~] # dumpe2fs /dev/md0 | grep -i superblock\ndumpe2fs 1.41.4 (27-Jan-2009)\n  Primary superblock at 0, Group descriptors at 1-233\n  Backup superblock at 32768, Group descriptors at 32769-33001\n  Backup superblock at 98304, Group descriptors at 98305-98537\n  Backup superblock at 163840, Group descriptors at 163841-164073\n...\n</code></pre>\n\n<p>When I tried the 2nd value, it spit out all sorts of errors, here's a snippit of them:</p>\n\n<pre><code>[~] # e2fsck -b 98304 /dev/md0\ne2fsck 1.41.4 (27-Jan-2009)\nSuperblock has an invalid journal (inode 8).\nanswer=1\n*** ext3 journal has been deleted - filesystem is now ext2 only ***\n\nBlock bitmap for group 1920 is not in group.  (block 223029632)\nanswer=1\nInode bitmap for group 1920 is not in group.  (block 971776033)\nanswer=1\nInode table for group 1920 is not in group.  (block 63045632)\nWARNING: SEVERE DATA LOSS POSSIBLE.\nanswer=1\nGroup descriptor 1920 marked uninitialized without feature set.\nanswer=1\nBlock bitmap for group 1921 is not in group.  (block 62916993)\nanswer=1\nInode bitmap for group 1921 is not in group.  (block 62916994)\nanswer=1\nGroup descriptor 1921 marked uninitialized without feature set.\nanswer=1\nBlock bitmap for group 1922 is not in group.  (block 1236358788)\nanswer=1\nGroup descriptor 1922 marked uninitialized without feature set.\nanswer=1\nInode table for group 1923 is not in group.  (block 2996300427)\nWARNING: SEVERE DATA LOSS POSSIBLE.\n...\nResize inode not valid.  answer=1\n/dev/md0 contains a file system with errors, check forced.\nPass 1: Checking inodes, blocks, and sizes\nRoot inode is not a directory.  answer=1\nInode 241 has EXTENTS_FL flag set on filesystem without extents support.\nanswer=1\nInode 263, i_blocks is 139224, should be 139056.  answer=1\nInode 264 has illegal block(s).  answer=1\nIllegal block #2060 (2770694604) in inode 264.  answer=1\nCLEARED.\nIllegal block #2061 (2431456604) in inode 264.  answer=1\nCLEARED.\n...\nToo many illegal blocks in inode 264.\nanswer=1\nInodes that were part of a corrupted orphan linked list found.  answer=1\nInode 8433 was part of the orphaned inode list.  answer=1\nFIXED.\nInode 8433 has imagic flag set.  answer=1\nInode 8433 has a extra size (48936) which is invalid\nanswer=1\nInode 8434 was part of the orphaned inode list.  answer=1\nFIXED.\nInode 8434 has imagic flag set.  answer=1\nInode 8434 has a extra size (49000) which is invalid\n...\nInode 16369 is in use, but has dtime set.  answer=1\nInode 16369 has a extra size (35995) which is invalid\nanswer=1\nInode 16370 is in use, but has dtime set.  answer=1\nInode 16370 has imagic flag set.  answer=1\nInode 16370 has a extra size (26087) which is invalid\nanswer=1\nInode 16371 is in use, but has dtime set.  answer=1\nInode 16371 has imagic flag set.  answer=1\nInode 16371 has a extra size (1476) which is invalid\nanswer=1\nInode 16372 has EXTENTS_FL flag set on filesystem without extents support.\n...\nInode 16380 has EXTENTS_FL flag set on filesystem without extents support.\nanswer=1\nInode 16381 has EXTENTS_FL flag set on filesystem without extents support.\nanswer=1\nInode 16382 has EXTENTS_FL flag set on filesystem without extents support.\nanswer=1\nInode 16383 has EXTENTS_FL flag set on filesystem without extents support.\nanswer=1\nInode 16384 has EXTENTS_FL flag set on filesystem without extents support.\nanswer=1\nInode 16375 has compression flag set on filesystem without compression support.  answer=1\nInode 16375 has a bad extended attribute block 335544554.  answer=1\nInode 16375 has INDEX_FL flag set but is not a directory.\nanswer=1\nInode 16375, i_size is 4795354609508463505, should be 0.  answer=1\nInode 16375, i_blocks is 119976829646520, should be 0.  answer=1\nInode 16373 has compression flag set on filesystem without compression support.  answer=1\n...\nInode 49141 is in use, but has dtime set.  answer=1\nInode 49141 has a extra size (53130) which is invalid\nanswer=1\nInode 49142 is in use, but has dtime set.  answer=1\nInode 49142 has a extra size (15011) which is invalid\nanswer=1\nInode 49143 has EXTENTS_FL flag set on filesystem without extents support.\nanswer=1\nInode 49144 has EXTENTS_FL flag set on filesystem without extents support.\n...\n</code></pre>\n\n<p>I finally did a ctrl-C to stop it, afraid it was going to cause more problems. It then spit out this:</p>\n\n<pre><code>^CRecreate journalanswer=1\nCreating journal (32768 blocks): \n\n Done.\n\n*** journal has been re-created - filesystem is now ext3 again ***\n/dev/md0: e2fsck canceled.\n\n/dev/md0: ***** FILE SYSTEM WAS MODIFIED *****\n[~] #\n</code></pre>\n\n<p>Any help on what to try next would be appreciated.  I did have a couple questions about raid's and mdadm since I'm a newbie when it comes to them.</p>\n\n<ol>\n<li><p>what happens if you try to mount them out of order, ie\n<br>mdadm -CfR --assume-clean /dev/md0 -l 5 -n 3 /dev/<strong>sda3</strong> /dev/<strong>sdb3</strong> /dev/sdc3\n<br>instead of\n<br>mdadm -CfR --assume-clean /dev/md0 -l 5 -n 3 /dev/<strong>sdb3</strong> /dev/<strong>sda3</strong> /dev/sdc3</p></li>\n<li><p>and why, when I run the mdadm command, is my new drive \"sdb3\" the only one showing the message: <em>appears to contain an ext2fs file system</em>?  Could that be part of my problem?</p></li>\n</ol>\n\n<p>Thanks,\nRobert</p>\n"},{"id":217023,"title":"Add two new HDD in Raid 1","body":"<p>I just made a fresh install of Ubuntu 12.10 on a new SSD (<code>/dev/sda</code>). Now I would like to add two 1TB drives (<code>/dev/sdb /dev/sdc</code>) in RAID 1 but I'm pretty clueless on how to do it.</p>\n\n<p>This is what I came up with (but didn't work):</p>\n\n<p>First created partitions on the new drives:</p>\n\n<p>sudo fdisk /dev/sdb</p>\n\n<p>created primary partition and write table to disk</p>\n\n<p>After that:</p>\n\n<pre><code>sudo mdadm --create --level=1 --name=raidarray --raid-devices=2 /dev/sdb /dev/sdc\n</code></pre>\n\n<p>However this outputs:</p>\n\n<pre><code>mdadm: device /dev/sdb exists but is not an md array.\n</code></pre>\n\n<p>Can somebody point me in the right direction on how to do this?</p>\n\n<p>One bonus question: Would it be possible to reserve a part (same size of SSD) of one HDD  and create a RAID 1 with the SSD and part of the HDD. And if so, how and can I keep my installation?</p>\n"},{"id":247981,"title":"Software raid - mdadm - re-find my array","body":"<p>Description</p>\n\n<p>Today I plugged in another hard drive and unplugged my raid drives to ensure when I wiped the drive, I would not accidentally pick the wrong drives.</p>\n\n<p>Now that I have re-plugged in my drives, the software raid 1 array is no longer being mounted/recognized/found. Using disk utility, I could see that the drives are /dev/sda and /dev/sdb so I tried running <code>sudo mdadm -A /dev/sda /dev/sdb</code> Unfortunately I keep getting an error message stating <code>mdadm: device /dev/sda exists but is not an md array</code></p>\n\n<hr>\n\n<p>Specifications:</p>\n\n<p>OS: Ubuntu 12.04 LTS Desktop (64 bit)</p>\n\n<p>Drives: \n2 x 3TB WD Red (same models brand new)\nOS installed on third drive (64GB ssd) (many linux installs)</p>\n\n<p>Motherboard:\n<a href=\"http://www.xbitlabs.com/articles/mainboards/display/evga-p55-ftw_2.html\">P55 FTW</a></p>\n\n<p>Processor:\nIntel i7-870 <a href=\"http://pastebin.com/raw.php?i=W4zary6Y\">Full Specs</a></p>\n\n<hr>\n\n<p>Result of <code>sudo mdadm --assemble --scan</code></p>\n\n<p><code>mdadm: No arrays found in config file or automatically</code></p>\n\n<p>When I boot from recovery mode I get a zillion 'ata1 error' codes flying by for a very long time.</p>\n\n<p>Can anyone let me know the proper steps for recovering the array?</p>\n\n<p>I would be happy just recover the data if that is a possible alternative to rebuilding the array. I have read about '<a href=\"http://www.cgsecurity.org/wiki/TestDisk\">test disk</a>' and it states on the wiki that it can find lost partitions for Linux RAID md 0.9/1.0/1.1/1.2 but I am running mdadm version 3.2.5 it seems. Has anyone else had experience with using this to recover software raid 1 data?</p>\n\n<hr>\n\n<p>Result of <code>sudo mdadm --examine /dev/sd* | grep -E \"(^\\/dev|UUID)\"</code></p>\n\n<pre><code>mdadm: No md superblock detected on /dev/sda.\nmdadm: No md superblock detected on /dev/sdb.\nmdadm: No md superblock detected on /dev/sdc1.\nmdadm: No md superblock detected on /dev/sdc3.\nmdadm: No md superblock detected on /dev/sdc5.\nmdadm: No md superblock detected on /dev/sdd1.\nmdadm: No md superblock detected on /dev/sdd2.\nmdadm: No md superblock detected on /dev/sde.\n/dev/sdc:\n/dev/sdc2:\n/dev/sdd:\n</code></pre>\n\n<hr>\n\n<p>Contents of mdadm.conf:</p>\n\n<pre><code># mdadm.conf\n#\n# Please refer to mdadm.conf(5) for information about this file.\n#\n\n# by default (built-in), scan all partitions (/proc/partitions) and all\n# containers for MD superblocks. alternatively, specify devices to scan, using\n# wildcards if desired.\n#DEVICE partitions containers\n\n# auto-create devices with Debian standard permissions\nCREATE owner=root group=disk mode=0660 auto=yes\n\n# automatically tag new arrays as belonging to the local system\nHOMEHOST &lt;system&gt;\n\n# instruct the monitoring daemon where to send mail alerts\nMAILADDR root\n\n# definitions of existing MD arrays\n\n# This file was auto-generated on Tue, 08 Jan 2013 19:53:56 +0000\n# by mkconf $Id$\n</code></pre>\n\n<hr>\n\n<p>Result of <code>sudo fdisk -l</code> as you can see sda and sdb are missing.</p>\n\n<pre><code>Disk /dev/sdc: 64.0 GB, 64023257088 bytes\n255 heads, 63 sectors/track, 7783 cylinders, total 125045424 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk identifier: 0x0009f38d\n\n   Device Boot      Start         End      Blocks   Id  System\n/dev/sdc1   *        2048     2000895      999424   82  Linux swap / Solaris\n/dev/sdc2         2002942    60594175    29295617    5  Extended\n/dev/sdc3        60594176   125044735    32225280   83  Linux\n/dev/sdc5         2002944    60594175    29295616   83  Linux\n\nDisk /dev/sdd: 60.0 GB, 60022480896 bytes\n255 heads, 63 sectors/track, 7297 cylinders, total 117231408 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk identifier: 0x58c29606\n\n   Device Boot      Start         End      Blocks   Id  System\n/dev/sdd1   *        2048      206847      102400    7  HPFS/NTFS/exFAT\n/dev/sdd2          206848   234455039   117124096    7  HPFS/NTFS/exFAT\n\nDisk /dev/sde: 60.0 GB, 60022480896 bytes\n255 heads, 63 sectors/track, 7297 cylinders, total 117231408 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk identifier: 0x00000000\n\nDisk /dev/sde doesn't contain a valid partition table\n</code></pre>\n\n<hr>\n\n<p>The output of dmesg | grep ata was <strong>very</strong> long so here is a link:\n<a href=\"http://pastebin.com/raw.php?i=H2dph66y\">http://pastebin.com/raw.php?i=H2dph66y</a></p>\n\n<hr>\n\n<p>The output of dmesg | grep ata | head -n 200 after setting bios to ahci and having to boot without those two discs.</p>\n\n<pre><code>[    0.000000]  BIOS-e820: 000000007f780000 - 000000007f78e000 (ACPI data)\n[    0.000000] Memory: 16408080k/18874368k available (6570k kernel code, 2106324k absent, 359964k reserved, 6634k data, 924k init)\n[    1.043555] libata version 3.00 loaded.\n[    1.381056] ata1: SATA max UDMA/133 abar m2048@0xfbff4000 port 0xfbff4100 irq 47\n[    1.381059] ata2: SATA max UDMA/133 abar m2048@0xfbff4000 port 0xfbff4180 irq 47\n[    1.381061] ata3: SATA max UDMA/133 abar m2048@0xfbff4000 port 0xfbff4200 irq 47\n[    1.381063] ata4: SATA max UDMA/133 abar m2048@0xfbff4000 port 0xfbff4280 irq 47\n[    1.381065] ata5: SATA max UDMA/133 abar m2048@0xfbff4000 port 0xfbff4300 irq 47\n[    1.381067] ata6: SATA max UDMA/133 abar m2048@0xfbff4000 port 0xfbff4380 irq 47\n[    1.381140] pata_acpi 0000:0b:00.0: PCI INT A -&gt; GSI 18 (level, low) -&gt; IRQ 18\n[    1.381157] pata_acpi 0000:0b:00.0: setting latency timer to 64\n[    1.381167] pata_acpi 0000:0b:00.0: PCI INT A disabled\n[    1.429675] ata_link link4: hash matches\n[    1.699735] ata1: SATA link down (SStatus 0 SControl 300)\n[    2.018981] ata2: SATA link down (SStatus 0 SControl 300)\n[    2.338066] ata3: SATA link down (SStatus 0 SControl 300)\n[    2.657266] ata4: SATA link down (SStatus 0 SControl 300)\n[    2.976528] ata5: SATA link up 1.5 Gbps (SStatus 113 SControl 300)\n[    2.979582] ata5.00: ATAPI: HL-DT-ST DVDRAM GH22NS50, TN03, max UDMA/100\n[    2.983356] ata5.00: configured for UDMA/100\n[    3.319598] ata6: SATA link up 3.0 Gbps (SStatus 123 SControl 300)\n[    3.320252] ata6.00: ATA-9: SAMSUNG SSD 830 Series, CXM03B1Q, max UDMA/133\n[    3.320258] ata6.00: 125045424 sectors, multi 16: LBA48 NCQ (depth 31/32), AA\n[    3.320803] ata6.00: configured for UDMA/133\n[    3.324863] Write protecting the kernel read-only data: 12288k\n[    3.374767] pata_marvell 0000:0b:00.0: PCI INT A -&gt; GSI 18 (level, low) -&gt; IRQ 18\n[    3.374795] pata_marvell 0000:0b:00.0: setting latency timer to 64\n[    3.375759] scsi6 : pata_marvell\n[    3.376650] scsi7 : pata_marvell\n[    3.376704] ata7: PATA max UDMA/100 cmd 0xdc00 ctl 0xd880 bmdma 0xd400 irq 18\n[    3.376707] ata8: PATA max UDMA/133 cmd 0xd800 ctl 0xd480 bmdma 0xd408 irq 18\n[    3.387938] sata_sil24 0000:07:00.0: version 1.1\n[    3.387951] sata_sil24 0000:07:00.0: PCI INT A -&gt; GSI 19 (level, low) -&gt; IRQ 19\n[    3.387974] sata_sil24 0000:07:00.0: Applying completion IRQ loss on PCI-X errata fix\n[    3.388621] scsi8 : sata_sil24\n[    3.388825] scsi9 : sata_sil24\n[    3.388887] scsi10 : sata_sil24\n[    3.388956] scsi11 : sata_sil24\n[    3.389001] ata9: SATA max UDMA/100 host m128@0xfbaffc00 port 0xfbaf0000 irq 19\n[    3.389004] ata10: SATA max UDMA/100 host m128@0xfbaffc00 port 0xfbaf2000 irq 19\n[    3.389007] ata11: SATA max UDMA/100 host m128@0xfbaffc00 port 0xfbaf4000 irq 19\n[    3.389010] ata12: SATA max UDMA/100 host m128@0xfbaffc00 port 0xfbaf6000 irq 19\n[    5.581907] ata9: SATA link up 3.0 Gbps (SStatus 123 SControl 0)\n[    5.618168] ata9.00: ATA-8: OCZ-REVODRIVE, 1.20, max UDMA/133\n[    5.618175] ata9.00: 117231408 sectors, multi 16: LBA48 NCQ (depth 31/32)\n[    5.658070] ata9.00: configured for UDMA/100\n[    7.852250] ata10: SATA link up 3.0 Gbps (SStatus 123 SControl 0)\n[    7.891798] ata10.00: ATA-8: OCZ-REVODRIVE, 1.20, max UDMA/133\n[    7.891804] ata10.00: 117231408 sectors, multi 16: LBA48 NCQ (depth 31/32)\n[    7.931675] ata10.00: configured for UDMA/100\n[   10.022799] ata11: SATA link down (SStatus 0 SControl 0)\n[   12.097658] ata12: SATA link down (SStatus 0 SControl 0)\n[   12.738446] EXT4-fs (sda3): mounted filesystem with ordered data mode. Opts: (null)\n</code></pre>\n\n<hr>\n\n<p>Smart tests on drives both came back 'healthy' however I cannot boot the machine with the drives plugged in when the machine is in AHCI mode (I dont know if this matters but these are 3tb WD reds). I hope this means the drives are fine as they were quite a bit to buy and brand new. Disk utility shows a massive grey 'unknown' shown below:</p>\n\n<p><img src=\"http://i.stack.imgur.com/styr7.png\" alt=\"Image of disk utility showing healthy drives\"></p>\n\n<p>I have since removed my RevoDrive to try and make things simpler/clearer.</p>\n\n<hr>\n\n<p>As far as I can tell, the motherboard doesn't have two controllers. Perhaps the Revodrive that I have since removed, which plugs in through pci was confusing things?</p>\n\n<hr>\n\n<p>Has anyone got any suggestions for how to recover the data from the drive rather than rebuilding the array? I.e. a step-by-step on using testdisk or some other data recovery program....</p>\n\n<hr>\n\n<p>I have tried putting the drives in another machine. I had the same issue where the machine would not get past the bios screen, but this one would constantly reboot itself. The only way to get the machine to boot would be to unplug the drives. I tried using different sata cables as well with no help. I did once manage to get it to discover the drive but again mdadm --examine revealed no block. Does this suggest my disks themselves are #@@#$#@ even though the short smart tests stated they were 'healthy'?</p>\n\n<hr>\n\n<p>It appears the drives are truly beyond rescue. I cant even format the volumes in disk utility. Gparted wont see the drives to put a partition table on. I cant even issue a secure erase command to fully reset the drives. It was definitely a software raid that I had set up <a href=\"http://askubuntu.com/questions/237825/would-mdadm-be-faster-than-fake-raid-adapter\">after discvering that the hardware raid I had initially tried was actually 'fake' raid and slower than software raid.</a></p>\n\n<p>Thank you for all your efforts to trying to help me. I guess the 'answer' is there is nothing you can do if you somehow manage to kill both your drives simultaneously.</p>\n\n<hr>\n\n<p>I retried SMART tests (this time in command line rather than disk utility) and the drives <strong>do</strong> respond successfully <strong>'without error'</strong>. However, I am unable to format the drives (using disk utility) or have them recognized by Gparted in that machine or another. I am also unable to run hdparm secure erase or security-set-password commands on the drives. Perhaps I need to dd /dev/null the entire drives? How on earth are they still responding to SMART but two computers are unable to do anything with them? I am running long smart tests on both drives now and will post results in 255 minutes (that's how long it said it was going to take).</p>\n\n<p>I have put the processor information up with the other technical specs (by motherboard etc) It turns out to be a pre-sandy architecture.</p>\n\n<hr>\n\n<p>Output of Extended SMART scan of one drive:</p>\n\n<pre><code>smartctl 5.41 2011-06-09 r3365 [x86_64-linux-3.2.0-36-generic] (local build)\nCopyright (C) 2002-11 by Bruce Allen, http://smartmontools.sourceforge.net\n\n=== START OF INFORMATION SECTION ===\nDevice Model:     WDC WD30EFRX-68AX9N0\nSerial Number:    WD-WMC1T1480750\nLU WWN Device Id: 5 0014ee 058d18349\nFirmware Version: 80.00A80\nUser Capacity:    3,000,592,982,016 bytes [3.00 TB]\nSector Sizes:     512 bytes logical, 4096 bytes physical\nDevice is:        Not in smartctl database [for details use: -P showall]\nATA Version is:   9\nATA Standard is:  Exact ATA specification draft version not indicated\nLocal Time is:    Sun Jan 27 18:21:48 2013 GMT\nSMART support is: Available - device has SMART capability.\nSMART support is: Enabled\n\n=== START OF READ SMART DATA SECTION ===\nSMART overall-health self-assessment test result: PASSED\n\nGeneral SMART Values:\nOffline data collection status:  (0x00) Offline data collection activity\n                    was never started.\n                    Auto Offline Data Collection: Disabled.\nSelf-test execution status:      (   0) The previous self-test routine completed\n                    without error or no self-test has ever \n                    been run.\nTotal time to complete Offline \ndata collection:        (41040) seconds.\nOffline data collection\ncapabilities:            (0x7b) SMART execute Offline immediate.\n                    Auto Offline data collection on/off support.\n                    Suspend Offline collection upon new\n                    command.\n                    Offline surface scan supported.\n                    Self-test supported.\n                    Conveyance Self-test supported.\n                    Selective Self-test supported.\nSMART capabilities:            (0x0003) Saves SMART data before entering\n                    power-saving mode.\n                    Supports SMART auto save timer.\nError logging capability:        (0x01) Error logging supported.\n                    General Purpose Logging supported.\nShort self-test routine \nrecommended polling time:    (   2) minutes.\nExtended self-test routine\nrecommended polling time:    ( 255) minutes.\nConveyance self-test routine\nrecommended polling time:    (   5) minutes.\nSCT capabilities:          (0x70bd) SCT Status supported.\n                    SCT Error Recovery Control supported.\n                    SCT Feature Control supported.\n                    SCT Data Table supported.\n\nSMART Attributes Data Structure revision number: 16\nVendor Specific SMART Attributes with Thresholds:\nID# ATTRIBUTE_NAME          FLAG     VALUE WORST THRESH TYPE      UPDATED  WHEN_FAILED RAW_VALUE\n  1 Raw_Read_Error_Rate     0x002f   200   200   051    Pre-fail  Always       -       0\n  3 Spin_Up_Time            0x0027   196   176   021    Pre-fail  Always       -       5175\n  4 Start_Stop_Count        0x0032   100   100   000    Old_age   Always       -       29\n  5 Reallocated_Sector_Ct   0x0033   200   200   140    Pre-fail  Always       -       0\n  7 Seek_Error_Rate         0x002e   200   200   000    Old_age   Always       -       0\n  9 Power_On_Hours          0x0032   100   100   000    Old_age   Always       -       439\n 10 Spin_Retry_Count        0x0032   100   253   000    Old_age   Always       -       0\n 11 Calibration_Retry_Count 0x0032   100   253   000    Old_age   Always       -       0\n 12 Power_Cycle_Count       0x0032   100   100   000    Old_age   Always       -       29\n192 Power-Off_Retract_Count 0x0032   200   200   000    Old_age   Always       -       24\n193 Load_Cycle_Count        0x0032   200   200   000    Old_age   Always       -       4\n194 Temperature_Celsius     0x0022   121   113   000    Old_age   Always       -       29\n196 Reallocated_Event_Count 0x0032   200   200   000    Old_age   Always       -       0\n197 Current_Pending_Sector  0x0032   200   200   000    Old_age   Always       -       0\n198 Offline_Uncorrectable   0x0030   100   253   000    Old_age   Offline      -       0\n199 UDMA_CRC_Error_Count    0x0032   200   200   000    Old_age   Always       -       0\n200 Multi_Zone_Error_Rate   0x0008   200   200   000    Old_age   Offline      -       0\n\nSMART Error Log Version: 1\nNo Errors Logged\n\nSMART Self-test log structure revision number 1\nNum  Test_Description    Status                  Remaining  LifeTime(hours)  LBA_of_first_error\n# 1  Extended offline    Completed without error       00%       437         -\n# 2  Short offline       Completed without error       00%       430         -\n# 3  Extended offline    Aborted by host               90%       430         -\n\nSMART Selective self-test log data structure revision number 1\n SPAN  MIN_LBA  MAX_LBA  CURRENT_TEST_STATUS\n    1        0        0  Not_testing\n    2        0        0  Not_testing\n    3        0        0  Not_testing\n    4        0        0  Not_testing\n    5        0        0  Not_testing\nSelective self-test flags (0x0):\n  After scanning selected spans, do NOT read-scan remainder of disk.\nIf Selective self-test is pending on power-up, resume after 0 minute delay.\n</code></pre>\n\n<hr>\n\n<p>It said completed without error. Does that mean the drive should be fine or just that the test was able to complete? Should I start a new question as I'm more concerned about getting the use of the drives back rather than the data/raid array at this point...</p>\n\n<hr>\n\n<p>Well today I was looking through my filesystem to see if there was any data to keep before setting up centOS instead. I noticed a folder called dmraid.sil in my home folder. I am guessing this is from when I had initially set up the raid array with the fake raid controller? I had made sure to remove the device (quite some time ago way before this problem) and just before using mdadm to create 'software raid'. Is there any way I have missed a trick somewhere and this was somehow running 'fake' raid without the device and that is what this dmraid.sil folder is all about? So confused. There are files in there like sda.size sda_0.dat sda_0.offset etc. Any advice on what this folder represents would be helpful.</p>\n\n<hr>\n\n<p>Turns out the drives were locked! I unlocked them easily enough with hdparm command. This is probably what caused all the Input Output errors. Unfortunately I now have this problem:</p>\n\n<p><img src=\"http://i.stack.imgur.com/PribA.png\" alt=\"enter image description here\"></p>\n\n<p>I have managed to mount the md device. Is it possible to unplug one drive, format it to a normal drive and copy the data to that? Ive had enough 'fun' with raid and am going to go down an automated backups route with rsync I think. I want to ask before I do anything that may cause data integrity issues.</p>\n\n<hr>\n"}]}]